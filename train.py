import torch
import os
import json
import uuid
import argparse
import time
import numpy as np
from datetime import datetime, timedelta
from dotenv import load_dotenv
from torch.utils.data import DataLoader
from wiki_dataset import WikiEmbeddingDataset
from model import DPEncoder
from loss import DPLossV1, DPLossV2, DPLossV1Norm, DPLossV2Norm

def knn_recall_at_k(x, z, k=10, batch_size=1000):
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ KNN recall ê³„ì‚° (ë°°ì¹˜ ì²˜ë¦¬)"""
    n = x.shape[0]
    overlaps = []
    
    for i in range(0, n, batch_size):
        end_i = min(i + batch_size, n)
        batch_x = x[i:end_i]
        batch_z = z[i:end_i]
        
        # í˜„ì¬ ë°°ì¹˜ì— ëŒ€í•´ ì „ì²´ ë°ì´í„°ì…‹ê³¼ì˜ ê±°ë¦¬ ê³„ì‚°
        dx_batch = torch.cdist(batch_x, x)  # [batch_size, n]
        dz_batch = torch.cdist(batch_z, z)  # [batch_size, n]
        
        # k+1 nearest neighbors ì°¾ê¸° (ìê¸° ìì‹  í¬í•¨)
        _, idx_x_batch = dx_batch.topk(k+1, largest=False)
        _, idx_z_batch = dz_batch.topk(k+1, largest=False)
        
        # ìê¸° ìì‹  ì œì™¸ (ì²« ë²ˆì§¸ ìš”ì†ŒëŠ” í•­ìƒ ìê¸° ìì‹ )
        idx_x_batch = idx_x_batch[:, 1:].cpu().numpy()
        idx_z_batch = idx_z_batch[:, 1:].cpu().numpy()
        
        # ë°°ì¹˜ ë‚´ ê° ìƒ˜í”Œì— ëŒ€í•´ overlap ê³„ì‚°
        for a, b in zip(idx_x_batch, idx_z_batch):
            overlaps.append(len(set(a) & set(b)) / k)
    
    return np.mean(overlaps)

def calculate_recall_during_training(model, dataset, device, k=5, num_samples=1000):
    """í•™ìŠµ ì¤‘ recall ê³„ì‚° (ìƒ˜í”Œë§ëœ ë°ì´í„°ë¡œ)"""
    model.eval()
    
    # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ìƒ˜í”Œë§
    np.random.seed(42)  # ì¼ê´€ëœ ìƒ˜í”Œë§ì„ ìœ„í•´
    indices = np.random.choice(len(dataset), min(num_samples, len(dataset)), replace=False)
    X = torch.stack([dataset[i] for i in indices]).to(device)
    
    with torch.no_grad():
        Z = model(X)
    
    recall = knn_recall_at_k(X, Z, k, batch_size=500)
    model.train()  # ë‹¤ì‹œ training ëª¨ë“œë¡œ
    return recall

def parse_args():
    parser = argparse.ArgumentParser(description="Train Distance Preservation Encoder")
    parser.add_argument("--epochs", type=int, default=100, help="Number of epochs to train")
    parser.add_argument("--batch-size", type=int, default=512, help="Batch size")
    parser.add_argument("--save-interval", type=int, default=5, help="Save checkpoint every N epochs")
    parser.add_argument("--lr", type=float, default=1e-3, help="Learning rate")
    parser.add_argument("--latent-dim", type=int, default=512, help="Latent dimension")
    parser.add_argument("--loss-function", type=str, default="DPLossV2", 
                       choices=["DPLossV1", "DPLossV1Norm", "DPLossV2", "DPLossV2Norm"],
                       help="Loss function to use")
    parser.add_argument("--tau", type=float, default=1.0, help="Temperature for soft ranking (V2 losses)")
    parser.add_argument("--lambda-rank", type=float, default=1.0, help="Weight for rank loss")
    parser.add_argument("--lambda-pairdist", type=float, default=0.3, help="Weight for pair distance loss")
    return parser.parse_args()

# Load environment variables
load_dotenv()

# Parse command line arguments
args = parse_args()

# Get device from environment with fallback logic
device_env = os.getenv('DEVICE', 'cpu')
if device_env == 'cuda' and not torch.cuda.is_available():
    print("Warning: CUDA requested but not available. Falling back to CPU.")
    device = torch.device('cpu')
elif device_env == 'mps' and not torch.backends.mps.is_available():
    print("Warning: MPS requested but not available. Falling back to CPU.")
    device = torch.device('cpu')
elif device_env == 'auto':
    if torch.cuda.is_available():
        device = torch.device('cuda')
    elif torch.backends.mps.is_available():
        device = torch.device('mps')
    else:
        device = torch.device('cpu')
else:
    device = torch.device(device_env)

print(f"Using device: {device}")
data_path = "wikipedia-22-12-ko-embeddings-100k.parquet"

# í•™ìŠµ ì„¤ì • (ëª…ë ¹í–‰ ì¸ìˆ˜ ìš°ì„ , í™˜ê²½ë³€ìˆ˜ ëŒ€ì²´, ê¸°ë³¸ê°’ ìµœí›„)
batch_size = args.batch_size
input_dim = 1024  # ì„ë² ë”© ì°¨ì›
hidden_dims = [768]
latent_dim = args.latent_dim
k = 5
lambda_rank = args.lambda_rank
lambda_pairdist = args.lambda_pairdist
lr = args.lr
epochs = args.epochs
save_interval = args.save_interval
loss_function_name = args.loss_function
tau = args.tau

# í™˜ê²½ë³€ìˆ˜ì—ì„œ ì¶”ê°€ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
save_interval = int(os.getenv('SAVE_INTERVAL', save_interval))

print(f"Training Configuration:")
print(f"  - Epochs: {epochs}")
print(f"  - Batch size: {batch_size}")
print(f"  - Learning rate: {lr}")
print(f"  - Latent dimension: {latent_dim}")
print(f"  - Loss function: {loss_function_name}")
print(f"  - Save interval: every {save_interval} epochs")
print("Loading dataset...")
dataset = WikiEmbeddingDataset(data_path)
loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)
print(f"Dataset size: {len(dataset)}")
print("Dataset loaded")

# UUID ìƒì„± ë° ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ ìƒì„±
experiment_id = str(uuid.uuid4())
checkpoint_dir = os.path.join("checkpoints", experiment_id)
os.makedirs(checkpoint_dir, exist_ok=True)

print(f"Experiment ID: {experiment_id}")
print(f"Checkpoint directory: {checkpoint_dir}")

# ì‹¤í—˜ ì„¤ì • ì €ì¥
config = {
    "experiment_id": experiment_id,
    "timestamp": datetime.now().isoformat(),
    "batch_size": batch_size,
    "hidden_dims": hidden_dims,
    "latent_dim": latent_dim,
    "k": k,
    "lambda_rank": lambda_rank,
    "lambda_pairdist": lambda_pairdist,
    "lr": lr,
    "epochs": epochs,
    "save_interval": save_interval,
    "loss_function": loss_function_name,
    "tau": tau,
    "device": str(device),
    "data_path": data_path
}

with open(os.path.join(checkpoint_dir, "config.json"), "w") as f:
    json.dump(config, f, indent=2)



# ì‹¤ì œ ì…ë ¥ ì°¨ì› í™•ì¸
actual_input_dim = dataset[0].shape[0]
print(f"Actual input dimension: {actual_input_dim}")

model = DPEncoder(actual_input_dim, hidden_dims, latent_dim).to(device)

# Loss í•¨ìˆ˜ ë™ì  ì„ íƒ
loss_classes = {
    "DPLossV1": DPLossV1,
    "DPLossV1Norm": DPLossV1Norm,
    "DPLossV2": DPLossV2,
    "DPLossV2Norm": DPLossV2Norm
}

loss_class = loss_classes[loss_function_name]

# V2 ê³„ì—´ lossëŠ” tau íŒŒë¼ë¯¸í„°ê°€ í•„ìš”
if "V2" in loss_function_name:
    criterion = loss_class(k=k, lambda_rank=lambda_rank, lambda_pairdist=lambda_pairdist, tau=tau).to(device)
else:
    criterion = loss_class(k=k, lambda_rank=lambda_rank, lambda_pairdist=lambda_pairdist).to(device)

print(f"Using loss function: {loss_function_name}")
optimizer = torch.optim.Adam(model.parameters(), lr=lr)

# ì†ì‹¤ê°’ ê¸°ë¡ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸
train_history = {
    "epochs": [],
    "total_loss": [],
    "rank_loss": [],
    "pairdist_loss": [],
    "epoch_times": [],
    "epoch_timestamps": [],
    "cumulative_time": [],
    "training_start_time": datetime.now().isoformat(),
    "batch_times": [],
    "recall_history": [],  # ì—í¬í¬ë³„ recall ê°’
    "recall_epochs": []    # recallì´ ê³„ì‚°ëœ ì—í¬í¬ë“¤
}

# ì „ì²´ í•™ìŠµ ì‹œì‘ ì‹œê°„
training_start_time = time.time()
print(f"Training started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print("="*80)

for epoch in range(epochs):
    epoch_start_time = time.time()
    epoch_timestamp = datetime.now()
    
    model.train()
    total_loss, total_rank, total_pair = 0, 0, 0
    n = 0
    batch_times = []
    
    for batch_idx, batch in enumerate(loader):
        batch_start_time = time.time()
        
        x = batch.to(device)
        z = model(x)
        loss, rank_loss, pairdist_loss = criterion(x, z)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        batch_end_time = time.time()
        batch_time = batch_end_time - batch_start_time
        batch_times.append(batch_time)

        bs = x.size(0)
        total_loss += loss.item() * bs
        total_rank += rank_loss.item() * bs
        total_pair += (pairdist_loss.item() if isinstance(pairdist_loss, torch.Tensor) else pairdist_loss) * bs
        n += bs
    
    epoch_end_time = time.time()
    epoch_duration = epoch_end_time - epoch_start_time
    cumulative_time = epoch_end_time - training_start_time
    
    # ì—í¬í¬ë³„ í‰ê·  ì†ì‹¤ ê³„ì‚°
    avg_total_loss = total_loss / n
    avg_rank_loss = total_rank / n
    avg_pair_loss = total_pair / n
    
    # ë°°ì¹˜ ì‹œê°„ í†µê³„
    avg_batch_time = sum(batch_times) / len(batch_times)
    
    # ETA ê³„ì‚°
    if epoch > 0:
        avg_epoch_time = cumulative_time / (epoch + 1)
        eta_seconds = avg_epoch_time * (epochs - epoch - 1)
        eta = str(timedelta(seconds=int(eta_seconds)))
    else:
        eta = "Calculating..."
    
    print(f"[Epoch {epoch+1:03d}/{epochs}] Total: {avg_total_loss:.4f} | Rank: {avg_rank_loss:.4f} | PairDist: {avg_pair_loss:.4f}")
    print(f"  â”œâ”€ Epoch Time: {epoch_duration:.2f}s | Avg Batch: {avg_batch_time:.3f}s | ETA: {eta}")
    print(f"  â””â”€ Cumulative: {str(timedelta(seconds=int(cumulative_time)))} | Timestamp: {epoch_timestamp.strftime('%H:%M:%S')}")
    
    # ì†ì‹¤ê°’ ë° ì‹œê°„ ê¸°ë¡ (ë§¤ ì—í¬í¬ë§ˆë‹¤)
    train_history["epochs"].append(epoch + 1)
    train_history["total_loss"].append(avg_total_loss)
    train_history["rank_loss"].append(avg_rank_loss)
    train_history["pairdist_loss"].append(avg_pair_loss)
    train_history["epoch_times"].append(epoch_duration)
    train_history["epoch_timestamps"].append(epoch_timestamp.isoformat())
    train_history["cumulative_time"].append(cumulative_time)
    train_history["batch_times"].append({
        "epoch": epoch + 1,
        "avg_batch_time": avg_batch_time,
        "total_batches": len(batch_times),
        "batch_times": batch_times
    })
    
    # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ì§€ì •ëœ ê°„ê²© ë˜ëŠ” ë§ˆì§€ë§‰ ì—í¬í¬)
    should_save = (epoch + 1) % save_interval == 0 or (epoch + 1) == epochs
    
    if should_save:
        save_start_time = time.time()
        print(f"  â”œâ”€ Saving checkpoint at epoch {epoch+1}...")
        
        # KNN Recall ê³„ì‚°
        print(f"  â”œâ”€ Calculating KNN Recall@{k}...")
        recall_start_time = time.time()
        current_recall = calculate_recall_during_training(model, dataset, device, k=k)
        recall_duration = time.time() - recall_start_time
        print(f"  â”œâ”€ Recall@{k}: {current_recall:.4f} (calculated in {recall_duration:.2f}s)")
        
        # recall historyì— ì¶”ê°€
        train_history["recall_history"].append(current_recall)
        train_history["recall_epochs"].append(epoch + 1)
        
        checkpoint = {
            "epoch": epoch + 1,
            "model_state_dict": model.state_dict(),
            "optimizer_state_dict": optimizer.state_dict(),
            "train_history": train_history,
            "config": config
        }
        
        checkpoint_path = os.path.join(checkpoint_dir, f"checkpoint_epoch_{epoch+1:03d}.pth")
        torch.save(checkpoint, checkpoint_path)
        save_duration = time.time() - save_start_time
        print(f"  â””â”€ Checkpoint saved in {save_duration:.2f}s")
    
    # ìµœì‹  ëª¨ë¸ì€ ë§¤ ì—í¬í¬ë§ˆë‹¤ ì—…ë°ì´íŠ¸
    torch.save(model.state_dict(), os.path.join(checkpoint_dir, "latest_model.pth"))
    print()  # ì—í¬í¬ ê°„ êµ¬ë¶„ì„ ìœ„í•œ ë¹ˆ ì¤„

# ì „ì²´ í•™ìŠµ ì™„ë£Œ ì‹œê°„
training_end_time = time.time()
total_training_time = training_end_time - training_start_time

# ìµœì¢… í•™ìŠµ íˆìŠ¤í† ë¦¬ì— ì „ì²´ ì‹œê°„ ì •ë³´ ì¶”ê°€
train_history["training_end_time"] = datetime.now().isoformat()
train_history["total_training_time_seconds"] = total_training_time
train_history["total_training_time_formatted"] = str(timedelta(seconds=int(total_training_time)))

# ì‹œê°„ í†µê³„ ê³„ì‚°
total_epochs = len(train_history["epoch_times"])
avg_epoch_time = sum(train_history["epoch_times"]) / total_epochs
min_epoch_time = min(train_history["epoch_times"])
max_epoch_time = max(train_history["epoch_times"])

train_history["time_statistics"] = {
    "average_epoch_time": avg_epoch_time,
    "min_epoch_time": min_epoch_time,
    "max_epoch_time": max_epoch_time,
    "total_epochs": total_epochs
}

# ìµœì¢… í•™ìŠµ íˆìŠ¤í† ë¦¬ ì €ì¥
with open(os.path.join(checkpoint_dir, "train_history.json"), "w") as f:
    json.dump(train_history, f, indent=2)

print("="*80)
print(f"ğŸ‰ Training completed! ğŸ‰")
print(f"Total training time: {str(timedelta(seconds=int(total_training_time)))}")
print(f"Average time per epoch: {avg_epoch_time:.2f}s")
print(f"Fastest epoch: {min_epoch_time:.2f}s | Slowest epoch: {max_epoch_time:.2f}s")
print(f"All checkpoints saved in: {checkpoint_dir}")
print(f"Final model saved as: {os.path.join(checkpoint_dir, 'latest_model.pth')}")
print("="*80)

# ê¸°ì¡´ íŒŒì¼ëª…ìœ¼ë¡œë„ ì €ì¥ (í˜¸í™˜ì„±ì„ ìœ„í•´)
torch.save(model.state_dict(), "dpae.pth")